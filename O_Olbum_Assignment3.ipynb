{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment3 </br>\r\n",
    "### Orly Olbum </br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## P. 30 Problem 1 </br>\r\n",
    "### Tokenize the following sentence:  </br>\r\n",
    "### After sleeping for 2h, he decided to sleep for another two."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "sentence1 = \"After sleeping for 2h, he decided to sleep for another two.\"\r\n",
    "\r\n",
    "def tokenizeWithWhiteSpace(doc: str):\r\n",
    "    '''Tokenize a document only using white space,\r\n",
    "      Args\r\n",
    "        doc -- is one string to be parsed\r\n",
    "      Returns:\r\n",
    "        a list with elements being the string split by white space\r\n",
    "    '''\r\n",
    "    return doc.split()\r\n",
    "\r\n",
    "token1 = tokenizeWithWhiteSpace(sentence1)\r\n",
    "print(token1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['After', 'sleeping', 'for', '2h,', 'he', 'decided', 'to', 'sleep', 'for', 'another', 'two.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## P. 30 Problem 2 </br>\r\n",
    "### Assume that all article, pronouns, and prepositions are stop words. Perform a sensible stemming and case folding in the example of Exercise 1, and convert to a vector-space representation. Express your representation as a set of words with associated frequencies but no normalization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "from nltk.corpus import stopwords\r\n",
    "nltk.download('stopwords')\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import re"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orlyo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\orlyo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# print words that are not stop words\r\n",
    "tokens_without_sw = [word for word in token1 if not word in stopwords.words()]\r\n",
    "print(tokens_without_sw)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['After', 'sleeping', '2h,', 'decided', 'sleep', 'another', 'two.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# case folding: proper case\r\n",
    "def lowerIt(tokenList):\r\n",
    "    '''Lower case all tokens,\r\n",
    "      input:\r\n",
    "        tokenList: list of tokens \r\n",
    "      output:\r\n",
    "        a list of lower case tokens\r\n",
    "    '''\r\n",
    "    return [i.lower() for i in tokenList]\r\n",
    "\r\n",
    "lower = lowerIt(tokens_without_sw)\r\n",
    "print(lower)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['after', 'sleeping', '2h,', 'decided', 'sleep', 'another', 'two.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# stemming: consolidate words with the same root\r\n",
    "# Lets stem\r\n",
    "myPatts = [r\"(?<=.{2})ies\\Z\", r\"es\\Z\", r\"(?<![ha|wa|$i|$a])(s{1})\\Z\", r\"(?<=.{2})(ing)\\Z\", r\"ly\\Z\", r\"er\\Z\",r\"(?<=.{2})(ed)\\Z\"]\r\n",
    "# note 2: r\"(?<![$ha|$wa|$i|$a])(s{1})\\Z\" uses negative look behind to avoid trimming (\"has\", \"was\", \"is\", \"as\") it would leave wawas\r\n",
    "# other patterns make sure that words like ring and red do not get stemmed (the \"\".{2}\"\" part)\r\n",
    "# (?<=) is a positive lookbehind. it says to look to right of the pattern in the group (), and \r\n",
    "# do not match that patter, just match the regex after it. Here, I am making sure there are at least two characters\r\n",
    "# before we lop off an ing or ed.\r\n",
    "\r\n",
    "def myStemmer(myList, myPatts):\r\n",
    "    ''' Stem tokens in myList based on myPatts\r\n",
    "    \r\n",
    "    Args\r\n",
    "      myList -- list of tokens to stem\r\n",
    "      myPatts -- list of regex patterns to use for stemming\r\n",
    "        each pattern, if matched, lead to a deletion\r\n",
    "    Returns\r\n",
    "      A list of stemmed tokens\r\n",
    "      \r\n",
    "    Note: the order of patterns matters, eg taking `ing` off changes the end of the\r\n",
    "      token and thus the regex match\r\n",
    "    '''    \r\n",
    "    out = myList\r\n",
    "    for pat in myPatts:\r\n",
    "        out = [re.sub(pat, \"\", i) for i in out]\r\n",
    "    return out\r\n",
    "\r\n",
    "print(set(myStemmer(lower, myPatts)))\r\n",
    "per_change = round(100-len(set(myStemmer(lower, myPatts)))/len(set(lower)) *100, ndigits=1)\r\n",
    "print(f\"\\n {per_change} percent fewer terms\".format( ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'2h,', 'two.', 'anoth', 'decid', 'sleep', 'aft'}\n",
      "\n",
      " 14.3 percent fewer terms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# vector space representation\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "def myDocTFVectorizor(doc, returnList=False):\r\n",
    "    '''Create dictionary (or list) of term frequencies from a list of tokens in a doc\r\n",
    "    Args\r\n",
    "      doc: a list of tokens for the document\r\n",
    "      returnList: do you want a dict (default) or list (returnList=True)\r\n",
    "    Returns\r\n",
    "      Either a dict with key as term and count as the value\r\n",
    "      or a list of tuples with the first val the term, and second the count\r\n",
    "    '''\r\n",
    "    # Counter is like a dict, but we can increment values to keys\r\n",
    "    # that do not exist\r\n",
    "    out = Counter()\r\n",
    "    for token in doc:\r\n",
    "        # this is the magic, increment even if token was not seen yet!\r\n",
    "        out[token] += 1\r\n",
    "    if not returnList:\r\n",
    "        return out\r\n",
    "    # some people like lists, so there is that option too\r\n",
    "    elif returnList:\r\n",
    "        return [(key,val) for key,val in out.items()]\r\n",
    "\r\n",
    "#Keep order of terms\r\n",
    "output1 = myDocTFVectorizor(lower)\r\n",
    "output1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'after': 1,\n",
       "         'sleeping': 1,\n",
       "         '2h,': 1,\n",
       "         'decided': 1,\n",
       "         'sleep': 1,\n",
       "         'another': 1,\n",
       "         'two.': 1})"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "output2 = myDocTFVectorizor(lower, returnList=True)\r\n",
    "myVocab, myCounts = zip(*output2)\r\n",
    "print(list(zip(myVocab, myCounts)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('after', 1), ('sleeping', 1), ('2h,', 1), ('decided', 1), ('sleep', 1), ('another', 1), ('two.', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## P. 30 Problem 3 </br>\r\n",
    "### Consider a collection in which the words \"after,\" \"decided,\" and \"another,\" each occur in 16% of the documents. All other words occur in 4% of the documents. Create and idf-normalized representation of your answer in Exercise 2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# put lists into a dictionary\r\n",
    "original = {myVocab[i]: myCounts[i] for i in range(len(myVocab))}\r\n",
    "print(original)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'after': 1, 'sleeping': 1, '2h,': 1, 'decided': 1, 'sleep': 1, 'another': 1, 'two.': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "freq_list = [\"after\", \"decided\", \"another\"]\r\n",
    "\r\n",
    "# for keys from the original that are in the freq_list, multiply their respective values by 0.16\r\n",
    "# for all other keys that are not in the freq_list, multiply their values by 0.04\r\n",
    "for key in original:\r\n",
    "    if key in freq_list:\r\n",
    "        original[key] *= 0.16\r\n",
    "    else:\r\n",
    "        original[key] *= 0.04\r\n",
    "\r\n",
    "# normalized values\r\n",
    "print(original)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'after': 0.16, 'sleeping': 0.04, '2h,': 0.04, 'decided': 0.16, 'sleep': 0.04, 'another': 0.16, 'two.': 0.04}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## P. 30 Problem 5 </br>\r\n",
    "### Compute the cosine similarity between the vector pair (1,2,3,4,0,1,0) and (4,3,2,1,1,0,0). Repeat the same computation with the Jaccard coefficient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "vector1 = [1, 2, 3, 4, 0, 1, 0]\r\n",
    "vector2 = [4, 3, 2, 1, 1, 0, 0]\r\n",
    "\r\n",
    "# from lecture: cosine similarity\r\n",
    "def myCosineSim(vec1, vec2):\r\n",
    "    import math\r\n",
    "    import numpy as np\r\n",
    "    vec1n = np.array(vec1)\r\n",
    "    vec2n = np.array(vec2)\r\n",
    "    return (vec1n * vec2n).sum() / (math.sqrt(((vec1n)**2).sum())*math.sqrt(((vec2n)**2).sum()))\r\n",
    "\r\n",
    "myCosineSim(vector1, vector2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6451612903225807"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# from lecture: Jaccard coefficient\r\n",
    "\r\n",
    "def jaccard(b1, b2):\r\n",
    "    '''Calculate Jaccard index for two binary vector\r\n",
    "    expects np.ndarrays\r\n",
    "    '''\r\n",
    "    x = np.array(b1)\r\n",
    "    y = np.array(b2)\r\n",
    "    return (x*y).sum()/(x.sum(0) + y.sum(0) - (x*y).sum(0))\r\n",
    "\r\n",
    "jaccard(vector1, vector2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## P. 30 Problem 6 </br>\r\n",
    "### Normalize each of the vectors in Exercise 5 to unit norm. Compute the Euclidean distance between the pair of normalized vectors. What is the relationship between this Euclidean distance and the cosine similarity computed in Exercise 5?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# normalize the vectors to unit norm\r\n",
    "v1 = np.array(vector1)\r\n",
    "v2 = np.array(vector2)\r\n",
    "\r\n",
    "vector1_norm = v1 / np.sqrt(np.sum(v1 ** 2))\r\n",
    "vector2_norm = v2 / np.sqrt(np.sum(v2 ** 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# from lecture: Euclidean distance\r\n",
    "def myEuclideanDist(vec1, vec2):\r\n",
    "    import math\r\n",
    "    import numpy as np\r\n",
    "    vec1n = np.array(vec1)\r\n",
    "    vec2n = np.array(vec2)\r\n",
    "    return math.sqrt(((vec1n-vec2n)**2).sum())\r\n",
    "\r\n",
    "myEuclideanDist(vector1_norm, vector2_norm)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8424235391742319"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### While the Euclidean distance is the distance between two vectors calculated using their norms (difference), the cosine similarity is the distance between two vectors with respect to their magnitudes. Euclidean distance serves better for text classification whereas cosine similarity functions better for retrieving texts similar to the document of interest."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "41ae306d6a8704444463402130b411b22fb6af047cdef8a56e45f99fcb5b30fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}